
@article{ramdas_sequential_2015,
	title = {Sequential {Nonparametric} {Testing} with the {Law} of the {Iterated} {Logarithm}},
	url = {http://arxiv.org/abs/1506.03486},
	abstract = {Consider the problem of nonparametric two-sample mean testing, where we have access to i.i.d. samples from two multivariate distributions and wish to test whether they have the same mean. We propose a {\textbackslash}textit\{sequential\} test for this problem suitable for data-rich, memory-constrained situations. It is novel in several ways: it takes linear time and constant space to compute on the fly, and has robust high-dimensional statistical performance, including basically the same power guarantee (for a given false positive rate) as a batch/offline version of the test with the same computational constraints. Most notably, it has a distinct computational advantage over the batch test, because it accesses only as many samples as are required -- its stopping time is adaptive to the unknown difficulty of the problem. We analyze the test and prove these properties in a rigorously finite-sample fashion, using a novel uniform empirical Bernstein version of the law of the iterated logarithm (LIL), which may be of independent interest and allows analysis of sequential tests in a general framework. We demonstrate how to extend our ideas to nonparametric homogeneity and independence testing, and make a case for their even broader applicability.},
	urldate = {2015-11-20},
	journal = {arXiv:1506.03486 [cs, math, stat]},
	author = {Ramdas, Aaditya and Balsubramani, Akshay},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.03486},
	keywords = {Computer Science - Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv\:1506.03486 PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/JZ2TV64S/Ramdas and Balsubramani - 2015 - Sequential Nonparametric Testing with the Law of t.pdf:application/pdf;arXiv.org Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/NZGUV7RS/1506.html:text/html}
}

@article{ding_randomization_2015,
	title = {Randomization inference for treatment effect variation},
	copyright = {© 2015 Royal Statistical Society},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/rssb.12124/abstract},
	doi = {10.1111/rssb.12124},
	abstract = {Applied researchers are increasingly interested in whether and how treatment effects vary in randomized evaluations, especially variation that is not explained by observed covariates. We propose a model-free approach for testing for the presence of such unexplained variation. To use this randomization-based approach, we must address the fact that the average treatment effect, which is generally the object of interest in randomized experiments, actually acts as a nuisance parameter in this setting. We explore potential solutions and advocate for a method that guarantees valid tests in finite samples despite this nuisance. We also show how this method readily extends to testing for heterogeneity beyond a given model, which can be useful for assessing the sufficiency of a given scientific theory. We finally apply our method to the National Head Start impact study, which is a large-scale randomized evaluation of a Federal preschool programme, finding that there is indeed significant unexplained treatment effect variation.},
	language = {en},
	urldate = {2016-03-01},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Ding, Peng and Feller, Avi and Miratrix, Luke},
	month = jul,
	year = {2015},
	keywords = {Causal inference, Head Start, Heterogeneous treatment effect, Randomization test},
	pages = {n/a--n/a},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/UD4S9IFJ/Ding et al. - 2015 - Randomization inference for treatment effect varia.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/PQUP882D/abstract.html:text/html}
}

@article{imai_get-out--vote_2005,
	title = {Do {Get}-{Out}-the-{Vote} {Calls} {Reduce} {Turnout}? {The} {Importance} of {Statistical} {Methods} for {Field} {Experiments}},
	volume = {null},
	issn = {1537-5943},
	shorttitle = {Do {Get}-{Out}-the-{Vote} {Calls} {Reduce} {Turnout}?},
	url = {http://journals.cambridge.org/article_S0003055405051658},
	doi = {10.1017/S0003055405051658},
	abstract = {In their landmark study of a field experiment, Gerber and Green (2000) found that get-out-the-vote calls reduce turnout by five percentage points. In this article, I introduce statistical methods that can uncover discrepancies between experimental design and actual implementation. The application of this methodology shows that Gerber and Green's negative finding is caused by inadvertent deviations from their stated experimental protocol. The initial discovery led to revisions of the original data by the authors and retraction of the numerical results in their article. Analysis of their revised data, however, reveals new systematic patterns of implementation errors. Indeed, treatment assignments of the revised data appear to be even less randomized than before their corrections. To adjust for these problems, I employ a more appropriate statistical method and demonstrate that telephone canvassing increases turnout by five percentage points. This article demonstrates how statistical methods can find and correct complications of field experiments.},
	number = {02},
	urldate = {2015-11-16},
	journal = {American Political Science Review},
	author = {Imai, Kosuke},
	month = may,
	year = {2005},
	pages = {283--300},
	file = {Cambridge Journals Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/NJ64NAXP/displayAbstract.html:text/html;Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/F5IBDCCZ/Imai - 2005 - Do Get-Out-the-Vote Calls Reduce Turnout The Impo.pdf:application/pdf}
}

@article{matchett_detecting_2015,
	title = {Detecting the influence of rare stressors on rare species in {Yosemite} {National} {Park} using a novel stratified permutation test},
	volume = {5},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep10702},
	doi = {10.1038/srep10702},
	urldate = {2016-02-25},
	journal = {Scientific Reports},
	author = {Matchett, J. R. and Stark, Philip B. and Ostoja, Steven M. and Knapp, Roland A. and McKenny, Heather C. and Brooks, Matthew L. and Langford, William T. and Joppa, Lucas N. and Berlow, Eric L.},
	month = jun,
	year = {2015},
	pages = {10702},
	file = {srep10702.pdf:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/A3URRIT8/srep10702.pdf:application/pdf}
}

@article{holland_statistics_1986,
	title = {Statistics and {Causal} {Inference}},
	volume = {81},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2289064},
	doi = {10.2307/2289064},
	abstract = {Problems involving causal inference have dogged at the heels of statistics since its earliest days. Correlation does not imply causation, and yet causal conclusions drawn from a carefully designed experiment are often valid. What can a statistical model say about causation? This question is addressed by using a particular model for causal inference (Holland and Rubin 1983; Rubin 1974) to critique the discussions of other writers on causation and causal inference. These include selected philosophers, medical researchers, statisticians, econometricians, and proponents of causal modeling.},
	number = {396},
	urldate = {2015-11-16},
	journal = {Journal of the American Statistical Association},
	author = {Holland, Paul W.},
	year = {1986},
	pages = {945--960},
	file = {JSTOR Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/4KX2FT6J/Holland - 1986 - Statistics and Causal Inference.pdf:application/pdf}
}

@article{rosenbaum_central_1983,
	title = {The central role of the propensity score in observational studies for causal effects},
	volume = {70},
	issn = {0006-3444, 1464-3510},
	url = {http://biomet.oxfordjournals.org/content/70/1/41},
	doi = {10.1093/biomet/70.1.41},
	abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two- dimensional plot.},
	language = {en},
	number = {1},
	urldate = {2015-11-16},
	journal = {Biometrika},
	author = {Rosenbaum, Paul R. and Rubin, Donald B.},
	month = apr,
	year = {1983},
	keywords = {Covariance adjustment, Direct adjustment, Discriminant matching, Matched sampling, Nonrandomized study, Standardization, Stratification, Subclassification},
	pages = {41--55},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/VG2W3TCJ/Rosenbaum and Rubin - 1983 - The central role of the propensity score in observ.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/XQ8JVE34/41.html:text/html}
}

@article{hainmueller_hidden_2015,
	title = {The {Hidden} {American} {Immigration} {Consensus}: {A} {Conjoint} {Analysis} of {Attitudes} toward {Immigrants}},
	volume = {59},
	copyright = {©2014, Midwest Political Science Association},
	issn = {1540-5907},
	shorttitle = {The {Hidden} {American} {Immigration} {Consensus}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/ajps.12138/abstract},
	doi = {10.1111/ajps.12138},
	abstract = {Many studies have examined Americans' immigration attitudes. Yet prior research frequently confounds multiple questions, including which immigrants to admit and how many to admit. To isolate attitudes on the former question, we use a conjoint experiment that simultaneously tests the influence of nine immigrant attributes in generating support for admission. Drawing on a two-wave, population-based survey, we demonstrate that Americans view educated immigrants in high-status jobs favorably, whereas they view those who lack plans to work, entered without authorization, are Iraqi, or do not speak English unfavorably. Strikingly, Americans' preferences vary little with their own education, partisanship, labor market position, ethnocentrism, or other attributes. Beneath partisan divisions over immigration lies a broad consensus about who should be admitted to the country. The results are consistent with norms-based and sociotropic explanations of immigration attitudes. This consensus points to limits in both theories emphasizing economic and cultural threats, and sheds new light on an ongoing policy debate.},
	language = {en},
	number = {3},
	urldate = {2016-02-04},
	journal = {American Journal of Political Science},
	author = {Hainmueller, Jens and Hopkins, Daniel J.},
	month = jul,
	year = {2015},
	pages = {529--548},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/UJKFM4NR/Hainmueller and Hopkins - 2015 - The Hidden American Immigration Consensus A Conjo.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/87M78V8W/abstract.html:text/html}
}

@article{bardenet_concentration_2015,
	title = {Concentration inequalities for sampling without replacement},
	volume = {21},
	issn = {1350-7265},
	url = {http://arxiv.org/abs/1309.4029},
	doi = {10.3150/14-BEJ605},
	abstract = {Concentration inequalities quantify the deviation of a random variable from a fixed value. In spite of numerous applications, such as opinion surveys or ecological counting procedures, few concentration results are known for the setting of sampling without replacement from a finite population. Until now, the best general concentration inequality has been a Hoeffding inequality due to Serfling [Ann. Statist. 2 (1974) 39-48]. In this paper, we first improve on the fundamental result of Serfling [Ann. Statist. 2 (1974) 39-48], and further extend it to obtain a Bernstein concentration bound for sampling without replacement. We then derive an empirical version of our bound that does not require the variance to be known to the user.},
	number = {3},
	urldate = {2016-01-12},
	journal = {Bernoulli},
	author = {Bardenet, Rémi and Maillard, Odalric-Ambrym},
	month = aug,
	year = {2015},
	note = {arXiv: 1309.4029},
	keywords = {Mathematics - Statistics Theory},
	pages = {1361--1385},
	file = {arXiv\:1309.4029 PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/B5U6HPHP/Bardenet and Maillard - 2015 - Concentration inequalities for sampling without re.pdf:application/pdf;arXiv.org Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/ETWNTDTG/1309.html:text/html}
}

@article{hodges_estimates_1963,
	title = {Estimates of {Location} {Based} on {Rank} {Tests}},
	volume = {34},
	issn = {0003-4851, 2168-8990},
	url = {http://projecteuclid.org/euclid.aoms/1177704172},
	doi = {10.1214/aoms/1177704172},
	abstract = {A serious objection to many of the classical statistical methods based on linear models or normality assumptions is their vulnerability to gross errors. For certain testing problems this difficulty is successfully overcome by rank tests such as the two Wilcoxon tests or the Kruskal-Wallis H-test. Their power is more robust against gross errors than that of the tt- and FF-tests, and their efficiency loss is quite small even in the rare case in which the suspicion of the possibility of gross errors is unfounded. For the corresponding problems of point estimation a beginning has been made to attack the difficulty by modifying the classical estimates either through removal or Winsorization of outlying observations; see for example Tukey (1960) and Anscombe (1960). It is the purpose of the present paper to explore a different approach to these problems of point estimation. In Sections 2-5 point estimates of location or shift parameter are defined in terms of rank test statistics such as the Wilcoxon or normal scores statistic, which are successful in providing robust power for the corresponding testing problems. In Sections 6 and 7, certain regularity and invariance properties of these estimates are proved. The distributions of the estimates are shown in Section 8 to be symmetric with respect to the parameter being estimated--and hence in particular to be unbiased--if the underlying distribution of the observations on which the estimate is based is symmetric. Without this assumption, the estimates are shown in Section 9 to be either exactly or approximately median unbiased for small samples and in Section 10 to be approximately normally distributed about the true parameter value for large samples. The variance of this asymptotic distribution depends of course on the underlying distribution of the observations, so that the estimates are not "distribution-free." In Section 9 there is also established a close relationship between the estimates and the corresponding upper and lower confidence bound for the parameter at confidence level 12{\textbackslash}frac\{1\}\{2\}, with which the estimate coincide in many cases. Finally, in Section 11, it is proved that the asymptotic relative efficiency of the estimates to the classical linear estimates is the same as the Pitman efficiency of the rank tests on which they are based to the corresponding tt-tests.},
	language = {EN},
	number = {2},
	urldate = {2015-12-02},
	journal = {The Annals of Mathematical Statistics},
	author = {Hodges, J. L. and Lehmann, E. L.},
	month = jun,
	year = {1963},
	mrnumber = {MR152070},
	zmnumber = {0203.21105},
	pages = {598--611},
	file = {euclid.aoms.1177704172.pdf:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/HTHWCAZQ/euclid.aoms.1177704172.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/9VVGKJIV/1177704172.html:text/html}
}

@article{ramdas_adaptivity_2015,
	title = {Adaptivity and {Computation}-{Statistics} {Tradeoffs} for {Kernel} and {Distance} based {High} {Dimensional} {Two} {Sample} {Testing}},
	url = {http://arxiv.org/abs/1508.00655},
	abstract = {Nonparametric two sample testing is a decision theoretic problem that involves identifying differences between two random variables without making parametric assumptions about their underlying distributions. We refer to the most common settings as mean difference alternatives (MDA), for testing differences only in first moments, and general difference alternatives (GDA), which is about testing for any difference in distributions. A large number of test statistics have been proposed for both these settings. This paper connects three classes of statistics - high dimensional variants of Hotelling's t-test, statistics based on Reproducing Kernel Hilbert Spaces, and energy statistics based on pairwise distances. We ask the question: how much statistical power do popular kernel and distance based tests for GDA have when the unknown distributions differ in their means, compared to specialized tests for MDA? We formally characterize the power of popular tests for GDA like the Maximum Mean Discrepancy with the Gaussian kernel (gMMD) and bandwidth-dependent variants of the Energy Distance with the Euclidean norm (eED) in the high-dimensional MDA regime. Some practically important properties include (a) eED and gMMD have asymptotically equal power; furthermore they enjoy a free lunch because, while they are additionally consistent for GDA, they also have the same power as specialized high-dimensional t-test variants for MDA. All these tests are asymptotically optimal (including matching constants) under MDA for spherical covariances, according to simple lower bounds, (b) The power of gMMD is independent of the kernel bandwidth, as long as it is larger than the choice made by the median heuristic, (c) There is a clear and smooth computation-statistics tradeoff for linear-time, subquadratic-time and quadratic-time versions of these tests, with more computation resulting in higher power.},
	urldate = {2015-11-20},
	journal = {arXiv:1508.00655 [cs, math, stat]},
	author = {Ramdas, Aaditya and Reddi, Sashank J. and Poczos, Barnabas and Singh, Aarti and Wasserman, Larry},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.00655},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Theory, Computer Science - Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
	file = {arXiv\:1508.00655 PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/5HNQ2EBN/Ramdas et al. - 2015 - Adaptivity and Computation-Statistics Tradeoffs fo.pdf:application/pdf;arXiv.org Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/K3ND7WC4/1508.html:text/html}
}

@misc{_nonparametric_????,
	title = {Nonparametric tests for the mean of a non-negative population},
	url = {http://www.sciencedirect.com/science/article/pii/S0378375801002944},
	urldate = {2016-02-01},
	file = {Nonparametric tests for the mean of a non-negative population:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/FMR57ZHV/S0378375801002944.html:text/html}
}

@article{rosenbaum_covariance_2002,
	title = {Covariance {Adjustment} in {Randomized} {Experiments} and {Observational} {Studies}},
	volume = {17},
	issn = {0883-4237, 2168-8745},
	url = {http://projecteuclid.org/euclid.ss/1042727942},
	doi = {10.1214/ss/1042727942},
	abstract = {By slightly reframing the concept of covariance adjustment in randomized experiments, a method of exact permutation inference is derived that is entirely free of distributional assumptions and uses the random assignment of treatments as the "reasoned basis for inference.'' This method of exact permutation inference may be used with many forms of covariance adjustment, including robust regression and locally weighted smoothers. The method is then generalized to observational studies where treatments were not randomly assigned, so that sensitivity to hidden biases must be examined. Adjustments using an instrumental variable are also discussed. The methods are illustrated using data from two observational studies.},
	number = {3},
	urldate = {2016-02-03},
	journal = {Statistical Science},
	author = {Rosenbaum, Paul R.},
	month = aug,
	year = {2002},
	mrnumber = {MR1962487},
	keywords = {Covariance adjustment, matching, observational studies, Permutation inference, propensity score, randomization inference, sensitivity analysis},
	pages = {286--327},
	file = {euclid.ss.1042727942.pdf:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/KBXRSKVX/euclid.ss.1042727942.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/GCC3WQ6U/1042727942.html:text/html}
}

@article{blaker_confidence_2000,
	title = {Confidence curves and improved exact confidence intervals for discrete distributions},
	volume = {28},
	copyright = {Copyright © 2000 Statistical Society of Canada},
	issn = {1708-945X},
	url = {http://onlinelibrary.wiley.com/doi/10.2307/3315916/abstract},
	doi = {10.2307/3315916},
	abstract = {The author describes a method for improving standard “exact” confidence intervals in discrete distributions with respect to size while retaining correct level. The binomial, negative binomial, hypergeometric, and Poisson distributions are considered explicitly. Contrary to other existing methods, the author's solution possesses a natural nesting condition: if α {\textless} α', the 1 - α' confidence interval is included in the 1 - α interval. Nonparametric confidence intervals for a quantile are also considered.},
	language = {en},
	number = {4},
	urldate = {2016-01-28},
	journal = {Canadian Journal of Statistics},
	author = {Blaker, Helge},
	month = dec,
	year = {2000},
	keywords = {Acceptability, binomial distribution, hypergeometric distribution, negative binomial distribution, nested confidence regions, Poisson distribution, p-value},
	pages = {783--798},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/K5P5XDU2/Blaker - 2000 - Confidence curves and improved exact confidence in.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/IJTCXCFB/abstract.html:text/html}
}

@article{bickel_discussion_2009,
	title = {Discussion of: {Brownian} distance covariance},
	volume = {3},
	issn = {1932-6157},
	shorttitle = {Discussion of},
	url = {http://arxiv.org/abs/0912.3295},
	doi = {10.1214/09-AOAS312A},
	abstract = {Discussion on "Brownian distance covariance" by G{\textbackslash}'\{a\}bor J. Sz{\textbackslash}'\{e\}kely and Maria L. Rizzo [arXiv:1010.0297]},
	number = {4},
	urldate = {2016-01-19},
	journal = {The Annals of Applied Statistics},
	author = {Bickel, Peter J. and Xu, Ying},
	month = dec,
	year = {2009},
	note = {arXiv: 0912.3295},
	keywords = {Mathematics - Statistics Theory, Statistics - Applications},
	pages = {1266--1269},
	file = {arXiv\:0912.3295 PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/7MHQTN2A/Bickel and Xu - 2009 - Discussion of Brownian distance covariance.pdf:application/pdf;arXiv.org Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/MCK68XQI/0912.html:text/html}
}

@article{hainmueller_entropy_2011,
	title = {Entropy {Balancing} for {Causal} {Effects}: {A} {Multivariate} {Reweighting} {Method} to {Produce} {Balanced} {Samples} in {Observational} {Studies}},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Entropy {Balancing} for {Causal} {Effects}},
	url = {http://pan.oxfordjournals.org/content/early/2011/10/15/pan.mpr025},
	doi = {10.1093/pan/mpr025},
	abstract = {This paper proposes entropy balancing, a data preprocessing method to achieve covariate balance in observational studies with binary treatments. Entropy balancing relies on a maximum entropy reweighting scheme that calibrates unit weights so that the reweighted treatment and control group satisfy a potentially large set of prespecified balance conditions that incorporate information about known sample moments. Entropy balancing thereby exactly adjusts inequalities in representation with respect to the first, second, and possibly higher moments of the covariate distributions. These balance improvements can reduce model dependence for the subsequent estimation of treatment effects. The method assures that balance improves on all covariate moments included in the reweighting. It also obviates the need for continual balance checking and iterative searching over propensity score models that may stochastically balance the covariate moments. We demonstrate the use of entropy balancing with Monte Carlo simulations and empirical applications.},
	language = {en},
	urldate = {2015-11-16},
	journal = {Political Analysis},
	author = {Hainmueller, Jens},
	month = oct,
	year = {2011},
	pages = {mpr025},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/PGZWQUQ7/Hainmueller - 2011 - Entropy Balancing for Causal Effects A Multivaria.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/2ZXUX4AB/pan.html:text/html}
}

@article{szekely_brownian_2009,
	title = {Brownian distance covariance},
	volume = {3},
	issn = {1932-6157},
	url = {http://arxiv.org/abs/1010.0297},
	doi = {10.1214/09-AOAS312},
	abstract = {Distance correlation is a new class of multivariate dependence coefficients applicable to random vectors of arbitrary and not necessarily equal dimension. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but generalize and extend these classical bivariate measures of dependence. Distance correlation characterizes independence: it is zero if and only if the random vectors are independent. The notion of covariance with respect to a stochastic process is introduced, and it is shown that population distance covariance coincides with the covariance with respect to Brownian motion; thus, both can be called Brownian distance covariance. In the bivariate case, Brownian covariance is the natural extension of product-moment covariance, as we obtain Pearson product-moment covariance by replacing the Brownian motion in the definition with identity. The corresponding statistic has an elegantly simple computing formula. Advantages of applying Brownian covariance and correlation vs the classical Pearson covariance and correlation are discussed and illustrated.},
	number = {4},
	urldate = {2016-01-19},
	journal = {The Annals of Applied Statistics},
	author = {Székely, Gábor J. and Rizzo, Maria L.},
	month = dec,
	year = {2009},
	note = {arXiv: 1010.0297},
	keywords = {Statistics - Applications},
	pages = {1236--1265},
	file = {arXiv\:1010.0297 PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/GZ6P52VP/Székely and Rizzo - 2009 - Brownian distance covariance.pdf:application/pdf;arXiv.org Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/7KM7VUDZ/1010.html:text/html}
}

@article{freedman_histogram_1981,
	title = {On the histogram as a density estimator:{L} 2 theory},
	volume = {57},
	issn = {0044-3719, 1432-2064},
	shorttitle = {On the histogram as a density estimator},
	url = {http://link.springer.com/article/10.1007/BF01025868},
	doi = {10.1007/BF01025868},
	language = {en},
	number = {4},
	urldate = {2016-02-09},
	journal = {Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete},
	author = {Freedman, David and Diaconis, Persi},
	month = dec,
	year = {1981},
	keywords = {Mathematical and Computational Physics, Mathematical Biology in General, Operation Research/Decision Theory, Probability Theory and Stochastic Processes, Quantitative Finance, Statistics for Business/Economics/Mathematical Finance/Insurance},
	pages = {453--476},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/V3F8DFGJ/Freedman and Diaconis - 1981 - On the histogram as a density estimatorL 2 theory.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/3UXF7ED2/10.html:text/html}
}

@article{winkler_permutation_2014,
	title = {Permutation inference for the general linear model},
	volume = {92},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811914000913},
	doi = {10.1016/j.neuroimage.2014.01.060},
	abstract = {Permutation methods can provide exact control of false positives and allow the use of non-standard statistics, making only weak assumptions about the data. With the availability of fast and inexpensive computing, their main limitation would be some lack of flexibility to work with arbitrary experimental designs. In this paper we report on results on approximate permutation methods that are more flexible with respect to the experimental design and nuisance variables, and conduct detailed simulations to identify the best method for settings that are typical for imaging research scenarios. We present a generic framework for permutation inference for complex general linear models (glms) when the errors are exchangeable and/or have a symmetric distribution, and show that, even in the presence of nuisance effects, these permutation inferences are powerful while providing excellent control of false positives in a wide range of common and relevant imaging research scenarios. We also demonstrate how the inference on glm parameters, originally intended for independent data, can be used in certain special but useful cases in which independence is violated. Detailed examples of common neuroimaging applications are provided, as well as a complete algorithm – the “randomise” algorithm – for permutation inference with the glm.},
	urldate = {2016-01-12},
	journal = {NeuroImage},
	author = {Winkler, Anderson M. and Ridgway, Gerard R. and Webster, Matthew A. and Smith, Stephen M. and Nichols, Thomas E.},
	month = may,
	year = {2014},
	keywords = {General linear model, Multiple regression, Permutation inference, Randomise},
	pages = {381--397},
	file = {ScienceDirect Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/7NVJA2EG/Winkler et al. - 2014 - Permutation inference for the general linear model.pdf:application/pdf;ScienceDirect Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/SRPK6259/S1053811914000913.html:text/html}
}

@inproceedings{lewis_here_2011,
	title = {Here, there, and everywhere: correlated online behaviors can lead to overestimates of the effects of advertising},
	shorttitle = {Here, there, and everywhere},
	url = {http://dl.acm.org/citation.cfm?id=1963431},
	urldate = {2015-11-16},
	booktitle = {Proceedings of the 20th international conference on {World} wide web},
	publisher = {ACM},
	author = {Lewis, Randall A. and Rao, Justin M. and Reiley, David H.},
	year = {2011},
	pages = {157--166},
	file = {[PDF] from researchgate.net:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/AN67977P/Lewis et al. - 2011 - Here, there, and everywhere correlated online beh.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/BF8H9JQG/citation.html:text/html}
}

@misc{center_for_history_and_new_media_zotero_????,
	title = {Zotero {Quick} {Start} {Guide}},
	url = {http://zotero.org/support/quick_start_guide},
	author = {{Center for History and New Media}}
}

@article{berger_fourth_1997,
	title = {The {Fourth} {Moment} {Method}},
	volume = {26},
	issn = {0097-5397},
	url = {http://epubs.siam.org/doi/abs/10.1137/S0097539792240005},
	doi = {10.1137/S0097539792240005},
	abstract = {Higher moment analysis has typically been used to upper bound certain functions. In this paper, we introduce a new combinatorial method to lower bound the expectation of the absolute value of a random variable X by the expectation of a quartic in X. In the special case where we are looking at the absolute value of a (weighted) sum of \{-1,+1\} unbiased random variables, we achieve tight bounds, using only a fourth moment, for the total discrepancy of a set system. Because the fourth moment depends only on 4-wise independence, our bounds will hold over polynomially sized distributions, and so these bounds will be directly applicable in removing randomness to obtain NC algorithms. We obtain the first NC algorithms for the problems of total discrepancy, maximum acyclic subgraph, tournament ranking, the Gale--Berlekamp switching game, and edge discrepancy. We show that for most of these applications it is truly necessary to consider a fourth moment by exhibiting a 3-wise independent distribution which does not achieve the required bounds. Our method is strong enough to give a new combinatorial bound on tournament ranking.},
	number = {4},
	urldate = {2016-02-04},
	journal = {SIAM Journal on Computing},
	author = {Berger, B.},
	month = aug,
	year = {1997},
	pages = {1188--1207},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/HEAQ8NDD/Berger - 1997 - The Fourth Moment Method.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/THCRCBQP/S0097539792240005.html:text/html}
}

@article{besag_generalized_1989,
	title = {Generalized {Monte} {Carlo} {Significance} {Tests}},
	volume = {76},
	issn = {0006-3444},
	url = {http://www.jstor.org/stable/2336623},
	doi = {10.2307/2336623},
	abstract = {Simple Monte Carlo significance testing has many applications, particularly in the preliminary analysis of spatial data. The method requires the value of the test statistic to be ranked among a random sample of values generated according to the null hypothesis. However, there are situations in which a sample of values can only be conveniently generated using a Markov chain, initiated by the observed data, so that independence is violated. This paper describes two methods that overcome the problem of dependence and allow exact tests to be carried out. The methods are applied to the Rasch model, to the finite lattice Ising model and to the testing of association between spatial processes. Power is discussed in a simple case.},
	number = {4},
	urldate = {2015-11-20},
	journal = {Biometrika},
	author = {Besag, Julian and Clifford, Peter},
	year = {1989},
	pages = {633--642}
}

@article{abadie_large_2006,
	title = {Large {Sample} {Properties} of {Matching} {Estimators} for {Average} {Treatment} {Effects}},
	volume = {74},
	issn = {1468-0262},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0262.2006.00655.x/abstract},
	doi = {10.1111/j.1468-0262.2006.00655.x},
	abstract = {Matching estimators for average treatment effects are widely used in evaluation research despite the fact that their large sample properties have not been established in many cases. The absence of formal results in this area may be partly due to the fact that standard asymptotic expansions do not apply to matching estimators with a fixed number of matches because such estimators are highly nonsmooth functionals of the data. In this article we develop new methods for analyzing the large sample properties of matching estimators and establish a number of new results. We focus on matching with replacement with a fixed number of matches. First, we show that matching estimators are not N1/2-consistent in general and describe conditions under which matching estimators do attain N1/2-consistency. Second, we show that even in settings where matching estimators are N1/2-consistent, simple matching estimators with a fixed number of matches do not attain the semiparametric efficiency bound. Third, we provide a consistent estimator for the large sample variance that does not require consistent nonparametric estimation of unknown functions. Software for implementing these methods is available in Matlab, Stata, and R.},
	language = {en},
	number = {1},
	urldate = {2016-03-04},
	journal = {Econometrica},
	author = {Abadie, Alberto and Imbens, Guido W.},
	month = jan,
	year = {2006},
	keywords = {Average treatment effects, Matching estimators, potential outcomes, selection on observables, unconfoundedness},
	pages = {235--267},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/M5N68WCN/Abadie and Imbens - 2006 - Large Sample Properties of Matching Estimators for.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/4MMHU7N7/abstract.html:text/html}
}

@article{wang_nonparametric_2003,
	title = {Nonparametric tests for the mean of a non-negative population},
	volume = {110},
	issn = {0378-3758},
	url = {http://www.sciencedirect.com/science/article/pii/S0378375801002944},
	doi = {10.1016/S0378-3758(01)00294-4},
	abstract = {We construct level-α tests for testing the null hypothesis that the mean of a non-negative population falls below a prespecified nominal value. These tests make no assumption about the distribution function other than that it be supported on [0,∞). Simple tests are derived based on either the sample mean or the sample product. The nonparametric likelihood ratio test is also discussed in this context. We also derive the uniformly most powerful monotone (UMP) tests for special cases.},
	number = {1–2},
	urldate = {2016-02-01},
	journal = {Journal of Statistical Planning and Inference},
	author = {Wang, Weizhen and Zhao, Linda H.},
	month = jan,
	year = {2003},
	keywords = {Level-α test, Markov's inequality, Non-negative random variable, Nonparametric likelihood ratio test, UMP test},
	pages = {75--96},
	file = {ScienceDirect Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/2SEFNPUP/Wang and Zhao - 2003 - Nonparametric tests for the mean of a non-negative.pdf:application/pdf;ScienceDirect Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/DU47VB7H/S0378375801002944.html:text/html}
}

@article{commenges_transformations_2003,
	title = {Transformations which preserve exchangeability and application to permutation tests},
	volume = {15},
	issn = {1048-5252},
	url = {http://dx.doi.org/10.1080/1048525031000089310},
	doi = {10.1080/1048525031000089310},
	abstract = {Exchangeability of observations is a key condition for applying permutation tests. We characterize the linear transformations which preserve exchangeability, distinguishing second-moment exchangeability and global exchangeability; we also examine non-linear transformations. When exchangeability does not hold one may try to find a transformation which achieves approximate exchangeability; then an approximate permutation test can be done. More specifically, consider a statistic T = φ( Y ); it may be possible to find V such that [Ytilde] = V ( Y ) is exchangeable and to write T = φ¯([Ytilde]). In other cases we may be content that [Ytilde] has an exchangeable variance matrix, which we denote second-moment exchangeability. When seeking transformations towards exchangeability we show the privileged role of residuals. We show that exact permutation tests can be constructed for the normal linear model. Finally we suggest approximate permutation tests based on second-moment exchangeability. In the case of an intraclass correlation model, the transformation is simple to implement. We also give permutational moments of linear and quadratic forms and show how this can be used through Cornish-Fisher expansions.},
	number = {2},
	urldate = {2016-03-01},
	journal = {Journal of Nonparametric Statistics},
	author = {Commenges, Daniel},
	month = jan,
	year = {2003},
	pages = {171--185},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/BAXE2MGJ/Commenges - 2003 - Transformations which preserve exchangeability and.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/AI2QXEHS/1048525031000089310.html:text/html}
}

@article{jens_hainmueller_causal_2013,
	title = {Causal {Inference} in {Conjoint} {Analysis}: {Understanding} {Multidimensional} {Choices} {Via} {Stated} {Preference} {Experiments}},
	volume = {22},
	issn = {1047-1987},
	shorttitle = {Causal {Inference} in {Conjoint} {Analysis}},
	doi = {10.2139/ssrn.2231687},
	number = {1},
	journal = {Political Analysis},
	author = {Jens Hainmueller, Daniel J. Hopkins},
	year = {2013},
	file = {Causal Inference in Conjoint Analysis\: Understanding Multidimensional Choices Via Stated Preference Experiments:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/T3W5KTVE/256051925_Causal_Inference_in_Conjoint_Analysis_Understanding_Multidimensional_Choices_Via_Stat.html:text/html;Political Analysis-2014-Hainmueller-1-30.pdf:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/672NNIXX/Political Analysis-2014-Hainmueller-1-30.pdf:application/pdf}
}

@article{athey_recursive_2015,
	title = {Recursive {Partitioning} for {Heterogeneous} {Causal} {Effects}},
	url = {http://arxiv.org/abs/1504.01132},
	abstract = {In this paper we study the problems of estimating heterogeneity in causal effects in experimental or observational studies and conducting inference about the magnitude of the differences in treatment effects across subsets of the population. In applications, our method provides a data-driven approach to determine which subpopulations have large or small treatment effects and to test hypotheses about the differences in these effects. For experiments, our method allows researchers to identify heterogeneity in treatment effects that was not specified in a pre-analysis plan, without concern about invalidating inference due to multiple testing. In most of the literature on supervised machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal is to build a model of the relationship between a unit's attributes and an observed outcome. A prominent role in these methods is played by cross-validation which compares predictions to actual outcomes in test samples, in order to select the level of complexity of the model that provides the best predictive power. Our method is closely related, but it differs in that it is tailored for predicting causal effects of a treatment rather than a unit's outcome. The challenge is that the "ground truth" for a causal effect is not observed for any individual unit: we observe the unit with the treatment, or without the treatment, but not both at the same time. Thus, it is not obvious how to use cross-validation to determine whether a causal effect has been accurately predicted. We propose several novel cross-validation criteria for this problem and demonstrate through simulations the conditions under which they perform better than standard methods for the problem of causal effects. We then apply the method to a large-scale field experiment re-ranking results on a search engine.},
	urldate = {2016-03-02},
	journal = {arXiv:1504.01132 [stat]},
	author = {Athey, Susan and Imbens, Guido},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.01132},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1504.01132 PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/PAETGI9F/Athey and Imbens - 2015 - Recursive Partitioning for Heterogeneous Causal Ef.pdf:application/pdf;arXiv.org Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/PR42NKSS/1504.html:text/html}
}

@techreport{lewis_worn-out_2012,
	title = {Worn-{Out} or {Just} {Getting} {Started}? {The} {Impact} of {Frequency} in {Online} {Display} {Advertising}},
	shorttitle = {Worn-{Out} or {Just} {Getting} {Started}?},
	url = {https://www.aeaweb.org/aea/2015conference/program/retrieve.php?pdfid=1259},
	urldate = {2015-11-16},
	institution = {working paper},
	author = {Lewis, Randall},
	year = {2012},
	file = {[PDF] from aeaweb.org:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/36CZ9NCC/Lewis - 2012 - Worn-Out or Just Getting Started The Impact of Fr.pdf:application/pdf}
}

@article{ho_randomization_2006,
	title = {Randomization {Inference} {With} {Natural} {Experiments}},
	volume = {101},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1198/016214505000001258},
	doi = {10.1198/016214505000001258},
	abstract = {Since the 2000 U.S. Presidential election, social scientists have rediscovered a long tradition of research examining the effects of ballot format on voting. Using a new dataset collected by The New York Times, we investigate the causal effect of being listed on the first ballot page in the 2003 California gubernatorial recall election. California law mandates a unique randomization procedure of ballot order that, when appropriately modeled, can be used to approximate a classical randomized experiment in a real world setting. We apply randomization inference based on Fisher's exact test, which directly incorporates the exact randomization procedure and yields accurate nonparametric confidence intervals. Our results suggest that being listed on the first ballot page causes a statistically significant increase in vote shares for more than 40\% of the minor candidates, whereas there is no significant effect for the top two candidates. We also investigate how randomization inference differs from conventional estimators that do not fully incorporate California's complex treatment assignment mechanism. The results indicate appreciable differences between the two approaches.},
	number = {475},
	urldate = {2016-01-28},
	journal = {Journal of the American Statistical Association},
	author = {Ho, Daniel E. and Imai, Kosuke},
	month = sep,
	year = {2006},
	pages = {888--900},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/6M7UCEBA/Ho and Imai - 2006 - Randomization Inference With Natural Experiments.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/896USBRT/016214505000001258.html:text/html}
}

@article{aronow_cluster-robust_2013,
	title = {Cluster-{Robust} {Variance} {Estimation} for {Dyadic} {Data}},
	url = {http://arxiv.org/abs/1312.3398},
	abstract = {Dyadic data are common in the social sciences, although inference for such settings involves accounting for a complex clustering structure. Many analyses in the social sciences fail to account for the fact that multiple dyads share a member, and that errors are thus likely correlated across these dyads. We propose a nonparametric sandwich-type robust variance estimator for linear regression to account for such clustering in dyadic data. We enumerate conditions for estimator consistency. We also extend our results to repeated and weighted observations, including directed dyads and longitudinal data, and provide an implementation for generalized linear models such as logistic regression. We examine empirical performance with simulations and applications to international relations and speed dating.},
	urldate = {2016-02-26},
	journal = {arXiv:1312.3398 [stat]},
	author = {Aronow, Peter M. and Samii, Cyrus and Assenova, Valentina A.},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.3398},
	keywords = {Statistics - Applications, Statistics - Methodology},
	file = {arXiv\:1312.3398 PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/QUCQK6RE/Aronow et al. - 2013 - Cluster-Robust Variance Estimation for Dyadic Data.pdf:application/pdf;arXiv.org Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/33VDEIXN/1312.html:text/html}
}

@article{diamond_genetic_2012,
	title = {Genetic {Matching} for {Estimating} {Causal} {Effects}: {A} {General} {Multivariate} {Matching} {Method} for {Achieving} {Balance} in {Observational} {Studies}},
	volume = {95},
	issn = {0034-6535},
	shorttitle = {Genetic {Matching} for {Estimating} {Causal} {Effects}},
	url = {http://dx.doi.org/10.1162/REST_a_00318},
	doi = {10.1162/REST_a_00318},
	abstract = {This paper presents genetic matching, a method of multivariate matching that uses an evolutionary search algorithm to determine the weight each covariate is given. Both propensity score matching and matching based on Mahalanobis distance are limiting cases of this method. The algorithm makes transparent certain issues that all matching methods must confront. We present simulation studies that show that the algorithm improves covariate balance and that it may reduce bias if the selection on observables assumption holds. We then present a reanalysis of a number of data sets in the LaLonde (1986) controversy.},
	number = {3},
	urldate = {2015-11-16},
	journal = {Review of Economics and Statistics},
	author = {Diamond, Alexis and Sekhon, Jasjeet S.},
	month = oct,
	year = {2012},
	pages = {932--945},
	file = {Review of Economics and Statistics Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/E9QWHF2I/Diamond and Sekhon - 2012 - Genetic Matching for Estimating Causal Effects A .pdf:application/pdf;Review of Economics and Statistics Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/UF4RV9IA/REST_a_00318.html:text/html}
}

@article{hainmueller_validating_2015,
	title = {Validating vignette and conjoint survey experiments against real-world behavior},
	volume = {112},
	issn = {0027-8424},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4345583/},
	doi = {10.1073/pnas.1416587112},
	abstract = {Little evidence exists on whether preferences about hypothetical choices measured in a survey experiment are driven by the same structural determinants of the actual choices made in the real world. This study answers this question using a natural experiment as a behavioral benchmark. Comparing the results from conjoint and vignette experiments on which attributes of hypothetical immigrants generate support for naturalization with the outcomes of closely corresponding referendums in Switzerland, we find that the effects estimated from the surveys match the effects of the same attributes in the behavioral benchmark remarkably well. We also find that seemingly subtle differences in survey designs can produce significant differences in performance. Overall, the paired conjoint design performs the best., Survey experiments, like vignette and conjoint analyses, are widely used in the social sciences to elicit stated preferences and study how humans make multidimensional choices. However, there is a paucity of research on the external validity of these methods that examines whether the determinants that explain hypothetical choices made by survey respondents match the determinants that explain what subjects actually do when making similar choices in real-world situations. This study compares results from conjoint and vignette analyses on which immigrant attributes generate support for naturalization with closely corresponding behavioral data from a natural experiment in Switzerland, where some municipalities used referendums to decide on the citizenship applications of foreign residents. Using a representative sample from the same population and the official descriptions of applicant characteristics that voters received before each referendum as a behavioral benchmark, we find that the effects of the applicant attributes estimated from the survey experiments perform remarkably well in recovering the effects of the same attributes in the behavioral benchmark. We also find important differences in the relative performances of the different designs. Overall, the paired conjoint design, where respondents evaluate two immigrants side by side, comes closest to the behavioral benchmark; on average, its estimates are within 2\% percentage points of the effects in the behavioral benchmark.},
	number = {8},
	urldate = {2016-02-04},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Hainmueller, Jens and Hangartner, Dominik and Yamamoto, Teppei},
	month = feb,
	year = {2015},
	pmid = {25646415},
	pmcid = {PMC4345583},
	pages = {2395--2400},
	file = {PubMed Central Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/D494V4JR/Hainmueller et al. - 2015 - Validating vignette and conjoint survey experiment.pdf:application/pdf}
}

@article{samii_equivalencies_2012,
	title = {On equivalencies between design-based and regression-based variance estimators for randomized experiments},
	volume = {82},
	issn = {0167-7152},
	url = {http://www.sciencedirect.com/science/article/pii/S0167715211003452},
	doi = {10.1016/j.spl.2011.10.024},
	abstract = {This paper demonstrates that the randomization-based “Neyman” and constant-effects estimators for the variance of estimated average treatment effects are equivalent to a variant of the White “heteroskedasticity-robust” estimator and the homoskedastic ordinary least squares (OLS) estimator, respectively.},
	number = {2},
	urldate = {2016-02-26},
	journal = {Statistics \& Probability Letters},
	author = {Samii, Cyrus and Aronow, Peter M.},
	month = feb,
	year = {2012},
	keywords = {potential outcomes, randomized experiments, Robust variance estimators},
	pages = {365--370},
	file = {ScienceDirect Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/P76S8SXI/Samii and Aronow - 2012 - On equivalencies between design-based and regressi.pdf:application/pdf;ScienceDirect Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/WU5C2SDI/S0167715211003452.html:text/html}
}

@article{ding_paradox_2014,
	title = {A paradox from randomization-based causal inference},
	url = {http://arxiv.org/abs/1402.0142},
	abstract = {Under the potential outcomes framework, causal effects are defined as comparisons between the potential outcomes under treatment and control. Based on the treatment assignment mechanism in randomized experiments, Neyman and Fisher proposed two different approaches to test the null hypothesis of zero average causal effect (Neyman's null) and the null hypothesis of zero individual causal effects (Fisher's null), respectively. Apparently, Fisher's null implies Neyman's null by logic. It is for this reason surprising that, in actual completely randomized experiments, rejection of Neyman's null does not imply rejection of Fisher's null in many realistic situations including the case with constant causal effect. Both numerical examples and asymptotic analysis support this surprising phenomenon. Although the connection between Neymanian approach and the Wald test under the linear model has been established in the literature, we provide a new connection between the Fisher Randomization Test and Rao's score test, which offers a new perspective on this paradox. Further, we show that the paradox also exists in other commonly used experiments, such as stratified experiments, matched-pair experiments and factorial experiments.},
	urldate = {2016-03-01},
	journal = {arXiv:1402.0142 [math, stat]},
	author = {Ding, Peng},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.0142},
	keywords = {62A01, 62G20, 62K99, Mathematics - Statistics Theory},
	file = {arXiv\:1402.0142 PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/EF4PSGWT/Ding - 2014 - A paradox from randomization-based causal inferenc.pdf:application/pdf;arXiv.org Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/AID64K98/1402.html:text/html}
}

@techreport{dehejia_local_2015,
	type = {Working {Paper}},
	title = {From {Local} to {Global}: {External} {Validity} in a {Fertility} {Natural} {Experiment}},
	shorttitle = {From {Local} to {Global}},
	url = {http://www.nber.org/papers/w21459},
	abstract = {Experimental evidence on a range of interventions in developing countries is accumulating rapidly. Is it possible to extrapolate from an experimental evidence base to other locations of policy interest (from “reference” to “target” sites)? And which factors determine the accuracy of such an extrapolation? We investigate applying the Angrist and Evans (1998) natural experiment (the effect of boy-boy or girl-girl as the first two children on incremental fertility and mothers’ labor force participation) to data from International IPUMS on 166 country-year censuses. We define the external validity function with extrapolation error depending on covariate differences between reference and target locations, and find that smaller differences in geography, education, calendar year, and mothers’ labor force participation lead to lower extrapolation error. As experimental evidence accumulates, out-of-sample extrapolation error does not systematically approach zero if the available evidence base is naïvely extrapolated, but does if the external validity function is used to select the most appropriate reference context for a given target (although absolute error remains meaningful relative to the magnitude of the treatment effect). We also investigate where to locate experiments and the decision problem associated with extrapolating from existing evidence rather than running a new experiment at a target site.},
	number = {21459},
	urldate = {2015-11-16},
	institution = {National Bureau of Economic Research},
	author = {Dehejia, Rajeev and Pop-Eleches, Cristian and Samii, Cyrus},
	month = aug,
	year = {2015},
	file = {NBER Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/54NJBWQ6/Dehejia et al. - 2015 - From Local to Global External Validity in a Ferti.pdf:application/pdf}
}

@article{hainmueller_who_2013,
	title = {Who {Gets} a {Swiss} {Passport}? {A} {Natural} {Experiment} in {Immigrant} {Discrimination}},
	volume = {107},
	issn = {1537-5943},
	shorttitle = {Who {Gets} a {Swiss} {Passport}?},
	url = {http://journals.cambridge.org/article_S0003055412000494},
	doi = {10.1017/S0003055412000494},
	abstract = {We study discrimination against immigrants using microlevel data from Switzerland, where, until recently, some municipalities used referendums to decide on the citizenship applications of foreign residents. We show that naturalization decisions vary dramatically with immigrants’ attributes, which we collect from official applicant descriptions that voters received before each referendum. Country of origin determines naturalization success more than any other applicant characteristic, including language skills, integration status, and economic credentials. The average proportion of “no” votes is about 40\% higher for applicants from (the former) Yugoslavia and Turkey compared to observably similar applicants from richer northern and western European countries. Statistical and taste-based discrimination contribute to varying naturalization success; the rewards for economic credentials are higher for applicants from disadvantaged origins, and origin-based discrimination is much stronger in more xenophobic municipalities. Moreover, discrimination against specific immigrant groups responds dynamically to changes in the groups’ relative size.},
	number = {01},
	urldate = {2016-01-31},
	journal = {American Political Science Review},
	author = {Hainmueller, Jens and Hangartner, Dominik},
	month = feb,
	year = {2013},
	pages = {159--187},
	file = {Cambridge Journals Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/KDX3WDXW/displayAbstract.html:text/html;Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/WA6F4MNT/Hainmueller and Hangartner - 2013 - Who Gets a Swiss Passport A Natural Experiment in.pdf:application/pdf}
}

@article{imbens_nonparametric_2004,
	title = {Nonparametric {Estimation} of {Average} {Treatment} {Effects} {Under} {Exogeneity}: {A} {Review}},
	volume = {86},
	issn = {0034-6535},
	shorttitle = {Nonparametric {Estimation} of {Average} {Treatment} {Effects} {Under} {Exogeneity}},
	url = {http://dx.doi.org/10.1162/003465304323023651},
	doi = {10.1162/003465304323023651},
	abstract = {Recently there has been a surge in econometric work focusing on estimating average treatment effects under various sets of assumptions. One strand of this literature has developed methods for estimating average treatment effects for a binary treatment under assumptions variously described as exogeneity, unconfoundedness, or selection on observables. The implication of these assumptions is that systematic (for example, average or distributional) differences in outcomes between treated and control units with the same values for the covariates are attributable to the treatment. Recent analysis has considered estimation and inference for average treatment effects under weaker assumptions than typical of the earlier literature by avoiding distributional and functional-form assumptions. Various methods of semiparametric estimation have been proposed, including estimating the unknown regression functions, matching, methods using the propensity score such as weighting and blocking, and combinations of these approaches. In this paper I review the state of this literature and discuss some of its unanswered questions, focusing in particular on the practical implementation of these methods, the plausibility of this exogeneity assumption in economic applications, the relative performance of the various semiparametric estimators when the key assumptions (unconfoundedness and overlap) are satisfied, alternative estimands such as quantile treatment effects, and alternate methods such as Bayesian inference.},
	number = {1},
	urldate = {2016-01-28},
	journal = {Review of Economics and Statistics},
	author = {Imbens, Guido W.},
	month = feb,
	year = {2004},
	pages = {4--29},
	file = {Review of Economics and Statistics Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/8G6KGJAU/Imbens - 2004 - Nonparametric Estimation of Average Treatment Effe.pdf:application/pdf;Review of Economics and Statistics Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/829BZ9AM/003465304323023651.html:text/html}
}

@article{imbens_robust_2005,
	title = {Robust, accurate confidence intervals with a weak instrument: quarter of birth and education},
	volume = {168},
	issn = {1467-985X},
	shorttitle = {Robust, accurate confidence intervals with a weak instrument},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-985X.2004.00339.x/abstract},
	doi = {10.1111/j.1467-985X.2004.00339.x},
	abstract = {Summary.  An instrument or instrumental variable manipulates a treatment and affects the outcome only indirectly through its manipulation of the treatment. For instance, encouragement to exercise might increase cardiovascular fitness, but only indirectly to the extent that it increases exercise. If instrument levels are randomly assigned to individuals, then the instrument may permit consistent estimation of the effects caused by the treatment, even though the treatment assignment itself is far from random. For instance, one can conduct a randomized experiment assigning some subjects to ‘encouragement to exercise’ and others to ‘no encouragement’ but, for reasons of habit or taste, some subjects will not exercise when encouraged and others will exercise without encouragement; none-the-less, such an instrument aids in estimating the effect of exercise. Instruments that are weak, i.e. instruments that have only a slight effect on the treatment, present inferential problems. We evaluate a recent proposal for permutation inference with an instrumental variable in four ways: using Angrist and Krueger's data on the effects of education on earnings using quarter of birth as an instrument, following Bound, Jaeger and Baker in using simulated independent observations in place of the instrument in Angrist and Krueger's data, using entirely simulated data in which correct answers are known and finally using statistical theory to show that only permutation inferences maintain correct coverage rates. The permutation inferences perform well in both easy and hard cases, with weak instruments, as well as with long-tailed responses.},
	language = {en},
	number = {1},
	urldate = {2016-02-19},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Imbens, Guido W. and Rosenbaum, Paul R.},
	month = jan,
	year = {2005},
	keywords = {Hodges–Lehmann estimate, Instrumental variable, Observational study, Permutation test, Randomization test},
	pages = {109--126},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/DQ43XUH6/Imbens and Rosenbaum - 2005 - Robust, accurate confidence intervals with a weak .pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/TPPWKSK4/abstract.html:text/html}
}

@article{cochran_effectiveness_1968,
	title = {The {Effectiveness} of {Adjustment} by {Subclassification} in {Removing} {Bias} in {Observational} {Studies}},
	volume = {24},
	issn = {0006-341X},
	url = {http://www.jstor.org/stable/2528036},
	doi = {10.2307/2528036},
	abstract = {In some investigations, comparison of the means of a variate y in two study groups may be biased because y is related to a variable x whose distribution differs in the two groups. A frequently used device for trying to remove this bias is adjustment by subclassification. The range of x is divided into c subclasses. Weighted means of the subclass means of y are compared, using the same weights for each study group. The effectiveness of this procedure in removing bias depends on several factors, but for monotonic relations between y and x, an analytical approach suggests that for c = 2, 3, 4, 5, and 6 the percentages of bias removed are roughly 64\%, 79\%, 86\%, 90\%, and 92\%, respectively. These figures should also serve as a guide when x is an ordered classification (e.g. none, slight, moderate, severe) that can be regarded as a grouping of an underlying continuous variable. The extent to which adjustment reduces the sampling error of the estimated difference between the y means is also examined. An interesting side result is that for x normal, the percentage reduction in the bias of \${\textbackslash}bar x\_2\$-\${\textbackslash}bar x\_1\$ due to adjustment equals the percentage reduction in its variance. Under a simple mathematical model, errors of measurement in x reduce the amount of bias removed to a fraction 1/(1 + h) of its value, where h is the ratio of the variance of the errors of measurement to the variance of the correct measurements. Since ordered classifications are often used because x is difficult to measure, h may be substantial in such cases, though more information is needed on the values of h that are typical in practice.},
	number = {2},
	urldate = {2016-01-28},
	journal = {Biometrics},
	author = {Cochran, W. G.},
	year = {1968},
	pages = {295--313},
	file = {JSTOR Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/USR32PI8/Cochran - 1968 - The Effectiveness of Adjustment by Subclassificati.pdf:application/pdf}
}

@article{dehejia_causal_1999,
	title = {Causal {Effects} in {Nonexperimental} {Studies}: {Reevaluating} the {Evaluation} of {Training} {Programs}},
	volume = {94},
	issn = {0162-1459},
	shorttitle = {Causal {Effects} in {Nonexperimental} {Studies}},
	url = {http://www.jstor.org/stable/2669919},
	doi = {10.2307/2669919},
	abstract = {This article uses propensity score methods to estimate the treatment impact of the National Supported Work (NSW) Demonstration, a labor training program, on postintervention earnings. We use data from Lalonde's evaluation of nonexperimental methods that combine the treated units from a randomized evaluation of the NSW with nonexperimental comparison units drawn from survey datasets. We apply propensity score methods to this composite dataset and demonstrate that, relative to the estimators that Lalonde evaluates, propensity score estimates of the treatment impact are much closer to the experimental benchmark estimate. Propensity score methods assume that the variables associated with assignment to treatment are observed (referred to as ignorable treatment assignment, or selection on observables). Even under this assumption, it is difficult to control for differences between the treatment and comparison groups when they are dissimilar and when there are many preintervention variables. The estimated propensity score (the probability of assignment to treatment, conditional on preintervention variables) summarizes the preintervention variables. This offers a diagnostic on the comparability of the treatment and comparison groups, because one has only to compare the estimated propensity score across the two groups. We discuss several methods (such as stratification and matching) that use the propensity score to estimate the treatment impact. When the range of estimated propensity scores of the treatment and comparison groups overlap, these methods can estimate the treatment impact for the treatment group. A sensitivity analysis shows that our estimates are not sensitive to the specification of the estimated propensity score, but are sensitive to the assumption of selection on observables. We conclude that when the treatment and comparison groups overlap, and when the variables determining assignment to treatment are observed, these methods provide a means to estimate the treatment impact. Even though propensity score methods are not always applicable, they offer a diagnostic on the quality of nonexperimental comparison groups in terms of observable preintervention variables.},
	number = {448},
	urldate = {2015-11-16},
	journal = {Journal of the American Statistical Association},
	author = {Dehejia, Rajeev H. and Wahba, Sadek},
	year = {1999},
	keywords = {Causality},
	pages = {1053--1062},
	file = {JSTOR Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/PB54S9BZ/Dehejia and Wahba - 1999 - Causal Effects in Nonexperimental Studies Reevalu.pdf:application/pdf}
}

@article{aronow_sharp_2014,
	title = {Sharp bounds on the variance in randomized experiments},
	volume = {42},
	issn = {0090-5364, 2168-8966},
	url = {http://projecteuclid.org/euclid.aos/1400592645},
	doi = {10.1214/13-AOS1200},
	abstract = {We propose a consistent estimator of sharp bounds on the variance of the difference-in-means estimator in completely randomized experiments. Generalizing Robins [Stat. Med. 7 (1988) 773–785], our results resolve a well-known identification problem in causal inference posed by Neyman [Statist. Sci. 5 (1990) 465–472. Reprint of the original 1923 paper]. A practical implication of our results is that the upper bound estimator facilitates the asymptotically narrowest conservative Wald-type confidence intervals, with applications in randomized controlled and clinical trials.},
	language = {EN},
	number = {3},
	urldate = {2016-02-26},
	journal = {The Annals of Statistics},
	author = {Aronow, Peter M. and Green, Donald P. and Lee, Donald K. K.},
	month = jun,
	year = {2014},
	mrnumber = {MR3210989},
	zmnumber = {1305.62024},
	keywords = {Causal inference, finite populations, potential outcomes, randomized experiments, variance estimation},
	pages = {850--871},
	file = {Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/IJQFNKGC/1400592645.html:text/html}
}

@article{stark_risk-limiting_2009,
	title = {Risk-{Limiting} {Postelection} {Audits}: {Conservative} -{Values} {From} {Common} {Probability} {Inequalities}},
	volume = {4},
	issn = {1556-6013},
	shorttitle = {Risk-{Limiting} {Postelection} {Audits}},
	doi = {10.1109/TIFS.2009.2034190},
	abstract = {Postelection audits of a random sample of batches of ballots against a trustworthy audit trail can limit the risk of certifying an incorrect electoral outcome to alpha, guaranteeing that - if the apparent outcome is wrong - the chance of a full hand count of the audit trail is at least 1-alpha. Risk-limiting audits can be built as sequential tests that audit more batches until either 1) there is strong evidence that the outcome is correct, given the errors found, or 2) there has been a complete hand count. The P -value of the hypothesis that the outcome is wrong is the largest chance, for all scenarios in which the outcome is wrong, that overstatements of the margins between winners and losers would be ldquono largerrdquo than they were observed to be. Different definitions of ldquolargerrdquo give different P -values. A small P -value is strong evidence that the outcome is correct. This paper gives simple approaches to calculating a conservative P-value for several ways of summarizing overstatements and several ways of drawing the sample of batches to audit, emphasizing sampling with probability proportional to a bound up on the error in the pth audit batch (PPEB sampling). A P-value based on Markov's inequality applied to a martingale constructed from the data seems the most efficient among the methods discussed; there are plans to use it to audit contests in two California counties in November 2009.},
	number = {4},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Stark, P.B.},
	month = dec,
	year = {2009},
	keywords = {conservative p-values, Dvoretzky–Kiefer–Wolfowitz inequality, Hoeffding's inequality, hypothesis test, Markov inequality, Markov processes, Markov's inequality, martingale, monetary unit sampling (MUS), NEGEXP, politics, PPEB sampling, probability inequalities, probability proportional to an error bound (PPEB), probability proportional to size, random ballot batch sample, risk-limiting postelection audits, sampling methods, sequential test},
	pages = {1005--1014},
	file = {IEEE Xplore Abstract Record:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/FF7HZU25/abs_all.html:text/html;IEEE Xplore Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/R3VPF48A/Stark - 2009 - Risk-Limiting Postelection Audits Conservative -V.pdf:application/pdf}
}

@article{abadie_semiparametric_2003,
	title = {Semiparametric instrumental variable estimation of treatment response models},
	volume = {113},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407602002014},
	doi = {10.1016/S0304-4076(02)00201-4},
	abstract = {This article introduces a new class of instrumental variable (IV) estimators for linear and nonlinear treatment response models with covariates. The rationale for focusing on nonlinear models is that, if the dependent variable is binary or limited, or if the effect of the treatment varies with covariates, a nonlinear model is appropriate. In the spirit of Roehrig (Econometrica 56 (1988) 433), identification is attained nonparametrically and does not depend on the choice of the parametric specification for the response function of interest. One virtue of this approach is that it allows the researcher to construct estimators that can be interpreted as the parameters of a well-defined approximation to a treatment response function under functional form misspecification. In contrast to some usual IV models, heterogeneity of treatment effects is not restricted by the identification conditions. The ideas and estimators in this article are illustrated using IV to estimate the effects of 401(k) retirement programs on savings.},
	number = {2},
	urldate = {2015-11-16},
	journal = {Journal of Econometrics},
	author = {Abadie, Alberto},
	month = apr,
	year = {2003},
	keywords = {401(k), Compliers, Semiparametric estimation, Treatment effects},
	pages = {231--263},
	file = {1-s2.0-S0304407602002014-main.pdf:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/6NRNWS6G/1-s2.0-S0304407602002014-main.pdf:application/pdf;ScienceDirect Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/83DZ39GS/S0304407602002014.html:text/html}
}

@article{freedman_weighting_2008,
	title = {Weighting regressions by propensity scores},
	volume = {32},
	issn = {0193-841X},
	doi = {10.1177/0193841X08317586},
	abstract = {Regressions can be weighted by propensity scores in order to reduce bias. However, weighting is likely to increase random error in the estimates, and to bias the estimated standard errors downward, even when selection mechanisms are well understood. Moreover, in some cases, weighting will increase the bias in estimated causal parameters. If investigators have a good causal model, it seems better just to fit the model without weights. If the causal model is improperly specified, there can be significant problems in retrieving the situation by weighting, although weighting may help under some circumstances.},
	language = {eng},
	number = {4},
	journal = {Evaluation Review},
	author = {Freedman, David A. and Berk, Richard A.},
	month = aug,
	year = {2008},
	pmid = {18591709},
	keywords = {Causality, Models, Statistical, Observation, Regression Analysis, Research Design, Selection Bias},
	pages = {392--409}
}

@article{rothstein_teacher_2010,
	title = {Teacher {Quality} in {Educational} {Production}: {Tracking}, {Decay}, and {Student} {Achievement}},
	volume = {125},
	issn = {0033-5533, 1531-4650},
	shorttitle = {Teacher {Quality} in {Educational} {Production}},
	url = {http://qje.oxfordjournals.org/content/125/1/175},
	doi = {10.1162/qjec.2010.125.1.175},
	abstract = {Growing concerns over the inadequate achievement of U.S. students have led to proposals to reward good teachers and penalize (or fire) bad ones. The leading method for assessing teacher quality is “value added” modeling (YAM), which decomposes students' test scores into components attributed to student heterogeneity and to teacher quality. Implicit in the VAM approach are strong assumptions about the nature of the educational production function and the assignment of students to classrooms. In this paper, I develop falsification tests for three widely used VAM specifications, based on the idea that future teachers cannot influence students' past achievement. In data from North Carolina, each of the VAMs' exclusion restrictions is dramatically violated. In particular, these models indicate large “effects” of fifth grade teachers on fourth grade test score gains. I also find that conventional measures of individual teachers' value added fade out very quickly and are at best weakly related to long-run effects. I discuss implications for the use of VAMs as personnel tools.},
	language = {en},
	number = {1},
	urldate = {2016-02-27},
	journal = {The Quarterly Journal of Economics},
	author = {Rothstein, Jesse},
	month = feb,
	year = {2010},
	pages = {175--214},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/ISGN2BQR/Rothstein - 2010 - Teacher Quality in Educational Production Trackin.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/X3VKKIBB/175.html:text/html}
}

@article{kaplan_method_1987,
	title = {A {Method} of {One}-{Sided} {Nonparametric} {Inference} for the {Mean} of a {Nonnegative} {Population}},
	volume = {41},
	issn = {0003-1305},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.1987.10475470},
	doi = {10.1080/00031305.1987.10475470},
	abstract = {For a few values drawn independently at random from a nonnegative population, I present what I have found to be an effective nonparametric method of one-sided inference for the population mean. The method is based on a martingale argument and use of the Markov inequality.},
	number = {2},
	urldate = {2016-02-01},
	journal = {The American Statistician},
	author = {Kaplan, Harold M.},
	month = may,
	year = {1987},
	pages = {157--158},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/VEI3QRR8/Kaplan - 1987 - A Method of One-Sided Nonparametric Inference for .pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/DNATU7IS/00031305.1987.html:text/html}
}

@article{rosenbaum_sensitivity_2007,
	title = {Sensitivity {Analysis} for m-{Estimates}, {Tests}, and {Confidence} {Intervals} in {Matched} {Observational} {Studies}},
	volume = {63},
	copyright = {©2006, The International Biometric Society},
	issn = {1541-0420},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1541-0420.2006.00717.x/abstract},
	doi = {10.1111/j.1541-0420.2006.00717.x},
	abstract = {Summary Huber's m-estimates use an estimating equation in which observations are permitted a controlled level of influence. The family of m-estimates includes least squares and maximum likelihood, but typical applications give extreme observations limited weight. Maritz proposed methods of exact and approximate permutation inference for m-tests, confidence intervals, and estimators, which can be derived from random assignment of paired subjects to treatment or control. In contrast, in observational studies, where treatments are not randomly assigned, subjects matched for observed covariates may differ in terms of unobserved covariates, so differing outcomes may not be treatment effects. In observational studies, a method of sensitivity analysis is developed for m-tests, m-intervals, and m-estimates: it shows the extent to which inferences would be altered by biases of various magnitudes due to nonrandom treatment assignment. The method is developed for both matched pairs, with one treated subject matched to one control, and for matched sets, with one treated subject matched to one or more controls. The method is illustrated using two studies: (i) a paired study of damage to DNA from exposure to chromium and nickel and (ii) a study with one or two matched controls comparing side effects of two drug regimes to treat tuberculosis. The approach yields sensitivity analyses for: (i) m-tests with Huber's weight function and other robust weight functions, (ii) the permutational t-test which uses the observations directly, and (iii) various other procedures such as the sign test, Noether's test, and the permutation distribution of the efficient score test for a location family of distributions. Permutation inference with covariance adjustment is briefly discussed.},
	language = {en},
	number = {2},
	urldate = {2016-01-29},
	journal = {Biometrics},
	author = {Rosenbaum, Paul R.},
	month = jun,
	year = {2007},
	keywords = {m-estimate, Noether's estimate, Permutation test, Randomization test},
	pages = {456--464},
	file = {Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/G59JHCDG/full.html:text/html}
}

@techreport{deaton_instruments_2009,
	type = {Working {Paper}},
	title = {Instruments of development: {Randomization} in the tropics, and the search for the elusive keys to economic development},
	shorttitle = {Instruments of development},
	url = {http://www.nber.org/papers/w14690},
	abstract = {There is currently much debate about the effectiveness of foreign aid and about what kind of projects can engender economic development. There is skepticism about the ability of econometric analysis to resolve these issues, or of development agencies to learn from their own experience. In response, there is movement in development economics towards the use of randomized controlled trials (RCTs) to accumulate credible knowledge of what works, without over-reliance on questionable theory or statistical methods. When RCTs are not possible, this movement advocates quasi-randomization through instrumental variable (IV) techniques or natural experiments. I argue that many of these applications are unlikely to recover quantities that are useful for policy or understanding: two key issues are the misunderstanding of exogeneity, and the handling of heterogeneity. I illustrate from the literature on aid and growth. Actual randomization faces similar problems as quasi-randomization, notwithstanding rhetoric to the contrary. I argue that experiments have no special ability to produce more credible knowledge than other methods, and that actual experiments are frequently subject to practical problems that undermine any claims to statistical or epistemic superiority. I illustrate using prominent experiments in development. As with IV methods, RCT-based evaluation of projects is unlikely to lead to scientific progress in the understanding of economic development. I welcome recent trends in development experimentation away from the evaluation of projects and towards the evaluation of theoretical mechanisms.},
	number = {14690},
	urldate = {2016-01-22},
	institution = {National Bureau of Economic Research},
	author = {Deaton, Angus S.},
	month = jan,
	year = {2009},
	file = {NBER Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/3MI8N3AJ/Deaton - 2009 - Instruments of development Randomization in the t.pdf:application/pdf}
}

@article{ng_m_smoking_2014,
	title = {{SMoking} prevalence and cigarette consumption in 187 countries, 1980-2012},
	volume = {311},
	issn = {0098-7484},
	url = {http://dx.doi.org/10.1001/jama.2013.284692},
	doi = {10.1001/jama.2013.284692},
	abstract = {Importance 
Tobacco is a leading global disease risk factor. Understanding national trends in prevalence and consumption is critical for prioritizing action and evaluating tobacco control progress.Objective
To estimate the prevalence of daily smoking by age and sex and the number of cigarettes per smoker per day for 187 countries from 1980 to 2012.Design
Nationally representative sources that measured tobacco use (n = 2102 country-years of data) were systematically identified. Survey data that did not report daily tobacco smoking were adjusted using the average relationship between different definitions. Age-sex-country-year observations (n = 38 315) were synthesized using spatial-temporal gaussian process regression to model prevalence estimates by age, sex, country, and year. Data on consumption of cigarettes were used to generate estimates of cigarettes per smoker per day.Main Outcomes and Measures
Modeled age-standardized prevalence of daily tobacco smoking by age, sex, country, and year; cigarettes per smoker per day by country and year.Results
Global modeled age-standardized prevalence of daily tobacco smoking in the population older than 15 years decreased from 41.2\% (95\% uncertainty interval [UI], 40.0\%-42.6\%) in 1980 to 31.1\% (95\% UI, 30.2\%-32.0\%; P {\textless} .001) in 2012 for men and from 10.6\% (95\% UI, 10.2\%-11.1\%) to 6.2\% (95\% UI, 6.0\%-6.4\%; P {\textless} .001) for women. Global modeled prevalence declined at a faster rate from 1996 to 2006 (mean annualized rate of decline, 1.7\%; 95\% UI, 1.5\%-1.9\%) compared with the subsequent period (mean annualized rate of decline, 0.9\%; 95\% UI, 0.5\%-1.3\%; P = .003). Despite the decline in modeled prevalence, the number of daily smokers increased from 721 million (95\% UI, 700 million–742 million) in 1980 to 967 million (95\% UI, 944 million–989 million; P {\textless} .001) in 2012. Modeled prevalence rates exhibited substantial variation across age, sex, and countries, with rates below 5\% for women in some African countries to more than 55\% for men in Timor-Leste and Indonesia. The number of cigarettes per smoker per day also varied widely across countries and was not correlated with modeled prevalence.Conclusions and Relevance
Since 1980, large reductions in the estimated prevalence of daily smoking were observed at the global level for both men and women, but because of population growth, the number of smokers increased significantly. As tobacco remains a threat to the health of the world’s population, intensified efforts to control its use are needed.},
	number = {2},
	urldate = {2015-12-23},
	journal = {JAMA},
	author = {{Ng M} and {Freeman MK} and {Fleming TD} and {et al}},
	month = jan,
	year = {2014},
	pages = {183--192}
}

@article{stark_verifiable_2014,
	title = {Verifiable {European} {Elections}: {Risk}-limiting {Audits} for {D}’{Hondt} and {Its} {Relatives}},
	volume = {1},
	issn = {2328-2797},
	shorttitle = {Verifiable {European} {Elections}},
	url = {https://www.usenix.org/jets/issues/0301/stark},
	urldate = {2016-01-28},
	journal = {USENIX Journal of Election Technology and Systems (JETS)},
	author = {Stark, Philip B. and Teague, Vanessa},
	year = {2014},
	pages = {18--39},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/MCGBU7GH/Stark and Teague - 2014 - Verifiable European Elections Risk-limiting Audit.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/QVWUJGQN/stark.html:text/html}
}

@article{imbens_better_2010,
	title = {Better {LATE} {Than} {Nothing}: {Some} {Comments} on {Deaton} (2009) and {Heckman} and {Urzua} (2009)},
	volume = {48},
	issn = {0022-0515},
	shorttitle = {Better {LATE} {Than} {Nothing}},
	url = {http://www.jstor.org/stable/20778730},
	abstract = {Two recent papers, Deaton (2009) and Heckman and Urzua (2009), argue against what they see as an excessive and inappropriate use of experimental and quasi-experimental methods in empirical work in economics in the last decade. They specifically question the increased use of instrumental variables and natural experiments in labor economics and of randomized experiments in development economics. In these comments, I will make the case that this move toward shoring up the internal validity of estimates, and toward clarifying the description of the population these estimates are relevant for, has been important and beneficial in increasing the credibility of empirical work in economics. I also address some other concerns raised by the Deaton and Heckman—Urzua papers.},
	number = {2},
	urldate = {2016-01-22},
	journal = {Journal of Economic Literature},
	author = {Imbens, Guido W.},
	year = {2010},
	pages = {399--423},
	file = {JSTOR Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/GG4QSM6Q/Imbens - 2010 - Better LATE Than Nothing Some Comments on Deaton .pdf:application/pdf}
}

@article{newey_asymptotic_1994,
	title = {The {Asymptotic} {Variance} of {Semiparametric} {Estimators}},
	volume = {62},
	issn = {0012-9682},
	url = {http://www.jstor.org/stable/2951752},
	doi = {10.2307/2951752},
	abstract = {The purpose of this paper is the presentation of a general formula for the asymptotic variance of a semiparametric estimator. A particularly important feature of this formula is a way of accounting for the presence of nonparametric estimates of nuisance functions. The general form of an adjustment factor for nonparametric estimates is derived and analyzed. The usefulness of the formula is illustrated by deriving propositions on invariance of the limiting distribution with respect to the nonparametric estimator, conditions for nonparametric estimation to have no effect on the asymptotic distribution, and the form of a correction term for the presence of nonparametric projection and density estimators. Examples discussed are quasi-maximum likelihood estimation of index models, panel probit with semiparametric individual effects, average derivatives, and inverse density weighted least squares. The paper also develops a set of regularity conditions for the validity of the asymptotic variance formula. Primitive regularity conditions are derived for {\textless}tex-math{\textgreater}\${\textbackslash}sqrt\{n\}{\textbackslash}text\{-consistency\}\${\textless}/tex-math{\textgreater} and asymptotic normality for functions of series estimators of projections. Specific examples are polynomial estimators of average derivative and semiparametric panel probit models.},
	number = {6},
	urldate = {2016-03-03},
	journal = {Econometrica},
	author = {Newey, Whitney K.},
	year = {1994},
	pages = {1349--1382},
	file = {JSTOR Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/XGFWEAT3/Newey - 1994 - The Asymptotic Variance of Semiparametric Estimato.pdf:application/pdf}
}

@article{serfling_probability_1974,
	title = {Probability {Inequalities} for the {Sum} in {Sampling} without {Replacement}},
	volume = {2},
	issn = {0090-5364},
	url = {http://www.jstor.org/stable/2958379},
	abstract = {Upper bounds are established for the probability that, in sampling without replacement from a finite population, the sample sum exceeds its expected value by a specified amount. These are obtained as corollaries of two main results. Firstly, a useful upper bound is derived for the moment generating function of the sum, leading to an exponential probability inequality and related moment inequalities. Secondly, maximal inequalities are obtained, extending Kolmogorov's inequality and the Hajek-Renyi inequality. Compared to sampling with replacement, the results incorporate sharpenings reflecting the influence of the sampling fraction, n/N, where n denotes the sample size and N the population size. We go somewhat beyond previous work by Hoeffding (1963) and Sen (1970). As in the latter reference, martingale techniques are exploited. Applications to simple linear rank statistics are noted, dealing with the two-sample Wilcoxon statistic as an example. Finally, the question of sharpness of the exponential bounds is considered.},
	number = {1},
	urldate = {2016-01-12},
	journal = {The Annals of Statistics},
	author = {Serfling, R. J.},
	year = {1974},
	pages = {39--48},
	file = {JSTOR Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/WPSVKM7E/Serfling - 1974 - Probability Inequalities for the Sum in Sampling w.pdf:application/pdf}
}

@article{heller_consistent_2013,
	title = {A consistent multivariate test of association based on ranks of distances},
	volume = {100},
	issn = {0006-3444, 1464-3510},
	url = {http://arxiv.org/abs/1201.3522},
	doi = {10.1093/biomet/ass070},
	abstract = {We are concerned with the detection of associations between random vectors of any dimension. Few tests of independence exist that are consistent against all dependent alternatives. We propose a powerful test that is applicable in all dimensions and is consistent against all alternatives. The test has a simple form and is easy to implement. We demonstrate its good power properties in simulations and on examples.},
	number = {2},
	urldate = {2016-01-19},
	journal = {Biometrika},
	author = {Heller, Ruth and Heller, Yair and Gorfine, Malka},
	month = jun,
	year = {2013},
	note = {arXiv: 1201.3522},
	keywords = {Statistics - Methodology},
	pages = {503--510},
	file = {arXiv\:1201.3522 PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/QMUA6B93/Heller et al. - 2013 - A consistent multivariate test of association base.pdf:application/pdf;arXiv.org Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/TEUXZZVM/1201.html:text/html}
}

@article{abadie_failure_2008,
	title = {On the {Failure} of the {Bootstrap} for {Matching} {Estimators}},
	volume = {76},
	copyright = {© 2008 The Econometric Society},
	issn = {1468-0262},
	url = {http://onlinelibrary.wiley.com/doi/10.3982/ECTA6474/abstract},
	doi = {10.3982/ECTA6474},
	abstract = {Matching estimators are widely used in empirical economics for the evaluation of programs or treatments. Researchers using matching methods often apply the bootstrap to calculate the standard errors. However, no formal justification has been provided for the use of the bootstrap in this setting. In this article, we show that the standard bootstrap is, in general, not valid for matching estimators, even in the simple case with a single continuous covariate where the estimator is root-N consistent and asymptotically normally distributed with zero asymptotic bias. Valid inferential methods in this setting are the analytic asymptotic variance estimator of Abadie and Imbens (2006a) as well as certain modifications of the standard bootstrap, like the subsampling methods in Politis and Romano (1994).},
	language = {en},
	number = {6},
	urldate = {2016-02-19},
	journal = {Econometrica},
	author = {Abadie, Alberto and Imbens, Guido W.},
	month = nov,
	year = {2008},
	keywords = {Average treatment effects, bootstrap, matching},
	pages = {1537--1557},
	file = {Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/GH9ZB466/Abadie and Imbens - 2008 - On the Failure of the Bootstrap for Matching Estim.pdf:application/pdf;Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/7W92QKU9/abstract.html:text/html}
}

@article{gerber_social_2008,
	title = {Social {Pressure} and {Voter} {Turnout}: {Evidence} from a {Large}-{Scale} {Field} {Experiment}},
	volume = {102},
	issn = {0003-0554},
	shorttitle = {Social {Pressure} and {Voter} {Turnout}},
	url = {http://www.jstor.org/stable/27644496},
	abstract = {Voter turnout theories based on rational self-interested behavior generally fail to predict significant turnout unless they account for the utility that citizens receive from performing their civic duty. We distinguish between two aspects of this type of utility, intrinsic satisfaction from behaving in accordance with a norm and extrinsic incentives to comply, and test the effects of priming intrinsic motives and applying varying degrees of extrinsic pressure. A large-scale field experiment involving several hundred thousand registered voters used a series of mailings to gauge these effects. Substantially higher turnout was observed among those who received mailings promising to publicize their turnout to their household or their neighbors. These findings demonstrate the profound importance of social pressure as an inducement to political participation.},
	number = {1},
	urldate = {2016-01-22},
	journal = {The American Political Science Review},
	author = {Gerber, Alan S. and Green, Donald P. and Larimer, Christopher W.},
	year = {2008},
	pages = {33--48},
	file = {JSTOR Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/KSW7T32N/Gerber et al. - 2008 - Social Pressure and Voter Turnout Evidence from a.pdf:application/pdf}
}

@inproceedings{goel_anatomy_2010,
	title = {Anatomy of the {Long} {Tail}: {Ordinary} {People} with {Extraordinary} {Tastes}},
	shorttitle = {Anatomy of the {Long} {Tail}},
	abstract = {The success of “infinite-inventory ” retailers such as Amazon.com and Netflix has been ascribed to a “long tail ” phenomenon. To wit, while the majority of their inventory is not in high demand, in aggregate these “worst sellers, ” unavailable at limited-inventory competitors, generate a significant fraction of total revenue. The long tail phenomenon, however, is in principle consistent with two fundamentally different theories. The first, and more popular hypothesis, is that a majority of consumers consistently follow the crowds and only a minority have any interest in niche content; the second hypothesis is that everyone is a bit eccentric, consuming both popular and specialty products. Based on examining extensive data on user preferences for movies, music, Web search, and Web browsing, we find overwhelming support for the latter theory. However, the observed eccentricity is much less than what is predicted by a fully random model whereby every consumer makes his product choices independently and proportional to product popularity; so consumers do indeed exhibit at least some a priori propensity toward either the popular or the exotic. Our findings thus suggest an additional factor in the success of infinite-inventory retailers, namely, that tail availability may boost head sales by offering consumers the convenience of “one-stop shopping ” for both their mainstream and niche interests. This hypothesis is further supported by our theoretical analysis that presents a simple model in which shared inventory stores, such as Amazon Marketplace, gain a clear advantage by satisfying tail demand, helping to explain the emergence and increasing popularity of such retail arrangements. Hence, we believe that the return-oninvestment (ROI) of niche products goes beyond direct revenue, extending to second-order gains associated with increased consumer satisfaction and repeat patronage. More generally, our findings call into question the conventional wisdom that specialty products only appeal to a minority of consumers.},
	booktitle = {in {WSDM}},
	author = {Goel, Sharad and Broder, Andrei and Gabrilovich, Evgeniy and Pang, Bo},
	year = {2010},
	file = {Citeseer - Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/4SITPDJ4/Goel et al. - 2010 - Anatomy of the Long Tail Ordinary People with Ext.pdf:application/pdf;Citeseer - Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/M9BZPIX4/summary.html:text/html}
}

@article{cochran_effectiveness_1968-1,
	title = {The {Effectiveness} of {Adjustment} by {Subclassification} in {Removing} {Bias} in {Observational} {Studies}},
	volume = {24},
	issn = {0006-341X},
	url = {http://www.jstor.org/stable/2528036},
	doi = {10.2307/2528036},
	abstract = {In some investigations, comparison of the means of a variate y in two study groups may be biased because y is related to a variable x whose distribution differs in the two groups. A frequently used device for trying to remove this bias is adjustment by subclassification. The range of x is divided into c subclasses. Weighted means of the subclass means of y are compared, using the same weights for each study group. The effectiveness of this procedure in removing bias depends on several factors, but for monotonic relations between y and x, an analytical approach suggests that for c = 2, 3, 4, 5, and 6 the percentages of bias removed are roughly 64\%, 79\%, 86\%, 90\%, and 92\%, respectively. These figures should also serve as a guide when x is an ordered classification (e.g. none, slight, moderate, severe) that can be regarded as a grouping of an underlying continuous variable. The extent to which adjustment reduces the sampling error of the estimated difference between the y means is also examined. An interesting side result is that for x normal, the percentage reduction in the bias of \${\textbackslash}bar x\_2\$-\${\textbackslash}bar x\_1\$ due to adjustment equals the percentage reduction in its variance. Under a simple mathematical model, errors of measurement in x reduce the amount of bias removed to a fraction 1/(1 + h) of its value, where h is the ratio of the variance of the errors of measurement to the variance of the correct measurements. Since ordered classifications are often used because x is difficult to measure, h may be substantial in such cases, though more information is needed on the values of h that are typical in practice.},
	number = {2},
	urldate = {2016-01-28},
	journal = {Biometrics},
	author = {Cochran, W. G.},
	year = {1968},
	pages = {295--313},
	file = {JSTOR Full Text PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/3ZJMN53U/Cochran - 1968 - The Effectiveness of Adjustment by Subclassificati.pdf:application/pdf}
}

@article{gogate_studies_2012,
	title = {Studies in {Lower} {Bounding} {Probabilities} of {Evidence} using the {Markov} {Inequality}},
	url = {http://arxiv.org/abs/1206.5242},
	abstract = {Computing the probability of evidence even with known error bounds is NP-hard. In this paper we address this hard problem by settling on an easier problem. We propose an approximation which provides high confidence lower bounds on probability of evidence but does not have any guarantees in terms of relative or absolute error. Our proposed approximation is a randomized importance sampling scheme that uses the Markov inequality. However, a straight-forward application of the Markov inequality may lead to poor lower bounds. We therefore propose several heuristic measures to improve its performance in practice. Empirical evaluation of our scheme with state-of- the-art lower bounding schemes reveals the promise of our approach.},
	urldate = {2016-02-01},
	journal = {arXiv:1206.5242 [cs]},
	author = {Gogate, Vibhav and Bidyuk, Bozhena and Dechter, Rina},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.5242},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:1206.5242 PDF:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/FBS3NPZV/Gogate et al. - 2012 - Studies in Lower Bounding Probabilities of Evidenc.pdf:application/pdf;arXiv.org Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/62B3F2VG/1206.html:text/html}
}

@article{bardenet_concentration_2015-1,
	title = {Concentration inequalities for sampling without replacement},
	volume = {21},
	issn = {1350-7265},
	url = {http://projecteuclid.org/euclid.bj/1432732023},
	doi = {10.3150/14-BEJ605},
	abstract = {Concentration inequalities quantify the deviation of a random variable from a fixed value. In spite of numerous applications, such as opinion surveys or ecological counting procedures, few concentration results are known for the setting of sampling without replacement from a finite population. Until now, the best general concentration inequality has been a Hoeffding inequality due to Serfling [ Ann. Statist. 2 (1974) 39–48]. In this paper, we first improve on the fundamental result of Serfling [ Ann. Statist. 2 (1974) 39–48], and further extend it to obtain a Bernstein concentration bound for sampling without replacement. We then derive an empirical version of our bound that does not require the variance to be known to the user.},
	language = {EN},
	number = {3},
	urldate = {2016-01-12},
	journal = {Bernoulli},
	author = {Bardenet, Rémi and Maillard, Odalric-Ambrym},
	month = aug,
	year = {2015},
	mrnumber = {MR3352047},
	keywords = {Bernstein, concentration bounds, sampling without replacement, Serfling},
	pages = {1361--1385},
	file = {Snapshot:/Users/Kellie/Library/Application Support/Zotero/Profiles/248c8jev.default/zotero/storage/E9MPSTSP/1432732023.html:text/html}
}